{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook holds the starting point for your Pattern Assignment. I'll put the function stub at the top, but I have some potentially helpful examples down below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "\n",
    "from nltk.book import *\n",
    "from nltk import FreqDist\n",
    "from string import punctuation\n",
    "from collections import Counter\n",
    "\n",
    "from pprint import pprint # get some prettier printing of objects\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "sw = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_patterns(text)  :\n",
    "    \"\"\"\n",
    "        This function takes text as an input and returns a dictionary of statistics,\n",
    "        after cleaning the text. \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # We'll make things a big clearer by initializing the \n",
    "    # statistics here. These are placeholder values.\n",
    "    \n",
    "    total_tokens = 1\n",
    "    unique_tokens = 0\n",
    "    avg_token_len = 0.0\n",
    "    lex_diversity = 0.0\n",
    "    top_10 = Counter()\n",
    "    \n",
    "    # Do your tokenization and normalization here\n",
    "    \n",
    "    text = text.split()\n",
    "    \n",
    "    words = []\n",
    "    \n",
    "    for word in text:\n",
    "        if word.isalpha():\n",
    "            word = word.lower()\n",
    "            if word in sw:\n",
    "                continue\n",
    "            else:\n",
    "                words.append(word)\n",
    "        else:\n",
    "            continue\n",
    "    \n",
    "    # Calculate your statistics here\n",
    "    \n",
    "    total_tokens = len(words)\n",
    "    Freq_tokens = FreqDist(words)\n",
    "    unique_tokens = len(list(Freq_tokens.keys()))\n",
    "    all_word_length_list = []\n",
    "    for words in list(words):\n",
    "        all_word_length = len(words)\n",
    "        all_word_length_list.append(all_word_length)\n",
    "        avg_token_len = sum(all_word_length_list)/len(all_word_length_list)\n",
    "    lex_diversity = unique_tokens/total_tokens\n",
    "    top_10 = Freq_tokens.most_common(10)\n",
    "    \n",
    "    # Now we'll fill out the dictionary. \n",
    "    results = {'tokens':total_tokens,\n",
    "               'unique_tokens':unique_tokens,\n",
    "               'avg_token_length':avg_token_len,\n",
    "               'lexical_diversity':lex_diversity,\n",
    "               'top_10_tokens':top_10}\n",
    "\n",
    "    return(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "beowulf = open(\"beowulf.txt\",'r').read()\n",
    "big_word_file = open(\"big.txt\",'r').read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "beo_results = get_patterns(beowulf)\n",
    "big_results = get_patterns(big_word_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you've finished the assignment, these next cells should run without raising any assertion errors. It's always possible that _I_ have a mistake in my version, so if you think you've done everything correctly and you're still getting errors, let me know. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passed all assertion tests.\n"
     ]
    }
   ],
   "source": [
    "assert(beo_results['tokens']==9288)\n",
    "assert(beo_results['unique_tokens']==2970)\n",
    "assert(0.31 < beo_results['lexical_diversity'] < 0.32)\n",
    "assert(5.3 < beo_results['avg_token_length'] < 5.4)\n",
    "assert(len(beo_results['top_10_tokens'])==10)\n",
    "assert(beo_results['top_10_tokens'][4]==('thou',47))\n",
    "print(\"Passed all assertion tests.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(big_results['tokens']==410950)\n",
    "assert(big_results['unique_tokens']==25173)\n",
    "assert(0.06 < big_results['lexical_diversity'] < 0.065)\n",
    "assert(6.3 < big_results['avg_token_length'] < 6.4)\n",
    "assert(len(big_results['top_10_tokens'])==10)\n",
    "assert(big_results['top_10'][0]==('said',2946))\n",
    "assert(big_results['top_10'][8]==('upon',1088))\n",
    "print(\"Passed all assertion tests.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "All of the important bits of the assignment are above here. Everything below is designed to help you wrap your mind around what I'm asking for. \n",
    "\n",
    "\n",
    "### Dictionaries\n",
    "\n",
    "We'll talk about dictionary objects in class a bit, but think of them as a big bag of key-value pairs. The keys, which have to be unique, are like an address to the data that's being stored in the \"value\", which can be any kind of object. They're incredibly versatile and useful objects to learn about. Here's a [video](https://www.youtube.com/watch?v=daefaLgNkw0) that will help orient you to them. \n",
    "\n",
    "In the next couple of cells, I'll make a couple of dictionaries and show you some basic functionality with them.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "animal_weights = {\"mouse\":0.03,\"dog\":20,\"horse\":700,\"elephant\":4100,\"blue whale\":100000}\n",
    "\n",
    "animal_weights['dog']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in animal_weights :\n",
    "    print(key)\n",
    "    print(animal_weights[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will throw an error\n",
    "animal_weights['cat']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "animal_weights['cat'] = 3\n",
    "\n",
    "animal_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for animal, wt in animal_weights.items() :\n",
    "    print(f\"An average {animal} weighs {wt} kilograms ({wt/2.2:0.2f} lbs).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions\n",
    "\n",
    "Functions are a useful way to organize code and prevent yourself from making mistakes. There is a lot to learn about functions, but we'll just handle the basics here. Functions are declared with the `def` statement (for \"define\", I assume). The parentheses allow you to define the _parameters_ that are passed into the function. Here's a simple example that squares a number. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_square(x) :\n",
    "    return(x*x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_square(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_square(102)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_square(\"cat\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can add some error handling to make sure we only try to square a number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def square(x) :\n",
    "    if not str(x).isnumeric() :\n",
    "        raise ValueError(f\"Hey, {x} isn't a number!\")\n",
    "    else :\n",
    "        return(x*x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "square(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "square(\"cat\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's an example of a function that returns a dictionary based on an NLTK text being sent in. (Note that those texts are in lists.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_text(text) :\n",
    "    # Take in a text, return the first token, the 100th token, and the last token.\n",
    "    \n",
    "    first_token = text[0]\n",
    "    hundredth_token = text[99]\n",
    "    last_token = text[-1]\n",
    "    \n",
    "    ret_dict = {'first':first_token,\n",
    "                '100th':hundredth_token,\n",
    "                'last':last_token}\n",
    "    return(ret_dict)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(process_text(text1))\n",
    "print(process_text(text5))\n",
    "print(process_text(text8))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
