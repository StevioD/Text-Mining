{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Functions\n",
    "\n",
    "Most of the interesting work in this assignment will happen when you create your own file `text_functions.py`. This next cell will load the higher-level functions into the kernel. \n",
    "\n",
    "**Note**: When you submit this, leave the output of code _I've_ written printed to the screen. This will make it easier for me to check your work. If you print some large stuff to the screen, you can delete those cells or just suppress the printing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import reuters\n",
    "from text_functions import clean_tokenize, get_patterns, compare_texts, correction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll just test them out. We'll use information from the Reuters corpus. More information can be found [here](https://www.nltk.org/book/ch02.html) in section 1.4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = reuters.categories()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crop_cats = [\"barley\",\"corn\",\"cotton\",\"grain\",\"potato\",\"rye\",\"sugar\",\"wheat\"]\n",
    "mining_cats = [\"alum\",\"copper\",\"silver\",\"gold\",\"iron-steel\",\"tin\",\"zinc\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Reuters corpus has 1.3M articles arranged into these categories. Let's build some big sets of text based on these categories. Articles can be in multiple categories. (Quick: what type of corpus do we call that?) So we'll pull articles that are exclusively in one of our categories. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crop_articles = set()\n",
    "mining_articles = set()\n",
    "\n",
    "for cat in crop_cats : \n",
    "    for article in reuters.fileids(cat) : \n",
    "        crop_articles.add(article)\n",
    "        \n",
    "for cat in mining_cats : \n",
    "    for article in reuters.fileids(cat) : \n",
    "        mining_articles.add(article)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_both = crop_articles.intersection(mining_articles)\n",
    "crop_articles = crop_articles - in_both\n",
    "mining_articles = mining_articles - in_both"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crop_text = []\n",
    "mining_text = []\n",
    "\n",
    "for article in crop_articles :\n",
    "    # Categories are stored in the article in upper case\n",
    "    article_text = [w for w in reuters.words(article) if w != w.upper()]\n",
    "    crop_text.extend(article_text)\n",
    "\n",
    "for article in mining_articles :\n",
    "    # Categories are stored in the article in upper case\n",
    "    article_text = [w for w in reuters.words(article) if w != w.upper()]\n",
    "    mining_text.extend(article_text)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we're in a position to test our code! \n",
    "\n",
    "### Cleaning and Tokenizing\n",
    "\n",
    "First we'll clean and tokenize, sending one set of text in as a list and the other in as a string, just to make sure both options work. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "holder = crop_text\n",
    "crop_text = clean_tokenize(holder)\n",
    "crop_text_2 = clean_tokenize(holder,remove_sw=False,remove_non_alpha=False)\n",
    "mining_text = clean_tokenize(\" \".join(mining_text),remove_sw=True,lowercase=True,remove_non_alpha=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(len(crop_text)==69727)\n",
    "assert(len(mining_text)==31275)\n",
    "assert(len([w for w in crop_text if w != w.lower()])==0)\n",
    "assert(len([w for w in mining_text if w != w.lower()])==0)\n",
    "assert(len(crop_text_2) - len(crop_text)==42870)\n",
    "print(\"Passed all assertion tests!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Patterns in a Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_patterns(crop_text,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_patterns(mining_text,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing Corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_texts(crop_text,mining_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(isinstance(crop_text,(list)))\n",
    "corrected_words = dict()\n",
    "\n",
    "for word in crop_text[:1000] :\n",
    "    cw = correction(word)\n",
    "    if cw != word :\n",
    "        corrected_words[word] = cw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(corrected_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for w, cw in corrected_words.items() :\n",
    "    print(f\"{w} was corrected to {cw}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
