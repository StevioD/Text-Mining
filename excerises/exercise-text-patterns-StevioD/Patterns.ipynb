{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's calculate some basic statistics regarding a body of text. We'll start with reading Beowulf from a text file, since reading in data is a common use case when you're working with text data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "\n",
    "from nltk.book import *\n",
    "from string import punctuation\n",
    "from collections import Counter\n",
    "\n",
    "from pprint import pprint # get some prettier printing of objects\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "sw = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beowulf = open(\"beowulf.txt\",'r').read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As always, we start with tokenization and normalization. Create a new variable, `beo_clean` that splits on whitespace, casts to lowercase, removes any tokens where `isalpha` is false, and removes stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's rip through the basic statistics: \n",
    "\n",
    "1. Overall text length (number of tokens)\n",
    "1. Number of unique tokens\n",
    "1. Lexical diversity \n",
    "1. Average token length\n",
    "1. Distribution of token lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frequency Distributions\n",
    "\n",
    "Frequency distributions are *incredibly* useful early in a text analysis. Let's build one for `beo_clean` and look at the 30 most common tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beo_fd = FreqDist(beo_clean)\n",
    "\n",
    "beo_fd.most_common(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many tokens have a count of 1 and what fraction do they represent? These are also called \"hapaxes\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interesting, about half the words only appear once. \n",
    "\n",
    "Frequency distribution objects come along with a plot method that can be quite useful. Plot the first 25 tokens (example is at the end section 3.1 of [chapter 1 of the NLTK book](https://www.nltk.org/book/ch01.html). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beo_fd.plot(25, cumulative=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a number of useful methods available for the `FreqDist` object. They're summarized in [this table](https://www.nltk.org/book/ch01.html#tab-freqdist) in NLTK Chapter 1.\n",
    "\n",
    "\n",
    "Not many words are longer than 12 characters. Print them all to the screen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLTK Functions\n",
    "\n",
    "The NLKT library comes with a number of helpful functions. In order to take advantage of those, you'll need to run your list of tokens through `nltk.Text`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beo_nltk = nltk.Text(beo_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concordance\n",
    "\n",
    "Concordances give you words in context. Let's look at usages of \"hrothgar\", the [Danish king](https://en.wikipedia.org/wiki/Hrothgar) featured in the book.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beo_nltk.concordance(\"hrothgar\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'd need to know more Beowulf than I do to appreciate these words. Let's look at the example from the NLTK book, comparing the usage of \"monstrous\" between _Moby Dick_ and _Sense and Sensibility_. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text1.concordance(\"monstrous\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text2.concordance(\"monstrous\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, Austen uses the word in _Sense and Sensibility_ in a different way from our modern usage. \n",
    "\n",
    "Related to concordances is the concept of similarity. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beo_nltk.similar(\"son\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text1.similar(\"whale\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text2.similar(\"love\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collocations\n",
    "\n",
    "Collocations are words that tend to appear together in a text. There are a number of different methods of deciding which collocations are \"most interesting\". One option is simple frequency: the collocations that appear a lot are the most important. Another option is to look at how often the words occur together versus how often they appear alone, using a calculation called [Pointwise Mutual Information](https://en.wikipedia.org/wiki/Pointwise_mutual_information) (PMI). Either way, we'll need to bring in some technology from the NLTK library to make these work.\n",
    "\n",
    "It can be quite useful to limit to some sort of minimal frequency if you're using the PMI measure. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.collocations import *\n",
    "bigram_measures = nltk.collocations.BigramAssocMeasures()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beo_coll = BigramCollocationFinder.from_words(beo_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beo_coll.nbest(bigram_measures.raw_freq,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beo_coll.apply_freq_filter(3)\n",
    "beo_coll.nbest(bigram_measures.pmi,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You could also do the frequency collocation with bigrams and FreqDist\n",
    "beo_bigram_fd = FreqDist(nltk.bigrams(beo_clean))\n",
    "beo_bigram_fd.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Words Corpus\n",
    "\n",
    "NLTK includes a corpus of words. It will occasionally be useful for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk_words = nltk.corpus.words.words()\n",
    "len(nltk_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"happy\" in nltk_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"hppy\" in nltk_words"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
