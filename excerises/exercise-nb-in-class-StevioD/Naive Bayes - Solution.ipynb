{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes Classifiers\n",
    "A powerful and intutitive technique. File this one away, it'll often teach you a lot about a problem, even if it doesn't \"win\" the accuracy game. First some examples from NLTK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import names\n",
    "import random\n",
    "from collections import Counter\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create some labeled observations\n",
    "labeled_names = ([(name, 'male') for name in names.words('male.txt')] +\n",
    "                 [(name, 'female') for name in names.words('female.txt')])\n",
    "\n",
    "# shuffle so that we can have a training and test set\n",
    "random.shuffle(labeled_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the purposes of this toy example, we just use the last letters as our only feature\n",
    "def gender_features(word):\n",
    "    return {'last_letter': word[-1]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this next line, read a bit about what's going on with this classifier [here](http://www.nltk.org/book/ch06.html). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This line is super important to understand\n",
    "featuresets = [(gender_features(n), gender) for (n, gender) in labeled_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set, test_set = featuresets[500:], featuresets[:500]\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test vs train\n",
    "print(nltk.classify.accuracy(classifier, test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "our_class = \"\"\"Mary Peyten Xin Alexis John Brenden\n",
    "               Madeline Claire Diana August Jon\n",
    "               Brenna Hong-Shen Chris CJ Kristi\n",
    "               Apsara Mike Craig\"\"\".split() \n",
    "# Taking some liberties with Hong Shen to prevent splitting his name\n",
    "\n",
    "for student in our_class :\n",
    "    print(student + \" classified as \" + classifier.classify(gender_features(student)))\n",
    "\n",
    "print(1-5/len(our_class)) #64% accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looking at the counts by gender can be useful for\n",
    "# understanding priors.\n",
    "Counter([gender for name, gender in labeled_names])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's just look at all the features. Usually you'd only show a few\n",
    "classifier.show_most_informative_features(26)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's build up some data sets so we can do iterative improvements to our model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(labeled_names) # Use this to shuffle in place to build training and test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This next cell is worth understanding. Ask questions if it is opaque. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_size = 500\n",
    "devtest_size = 1000\n",
    "\n",
    "train_names = labeled_names[(test_size + devtest_size):]\n",
    "devtest_names = labeled_names[test_size:(test_size + devtest_size)]\n",
    "test_names = labeled_names[:test_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = [(gender_features(n), gender) for (n, gender) in train_names]\n",
    "devtest_set = [(gender_features(n), gender) for (n, gender) in devtest_names]\n",
    "test_set = [(gender_features(n), gender) for (n, gender) in test_names]\n",
    "\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "print(nltk.classify.accuracy(classifier, devtest_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors = []\n",
    "for (name, tag) in devtest_names:\n",
    "    guess = classifier.classify(gender_features(name))\n",
    "    if guess != tag:\n",
    "        errors.append( (tag, guess, name) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the results of the cells below, and form some hypotheses of additional features to add. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (tag, guess, name) in sorted(errors):\n",
    "    print(f'correct={tag:<8} guess={guess:<8s} name={name:<30}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, look at the names that are being missed and see if you can add some features that will improve our accuracy. Some potential options:\n",
    "\n",
    "* Specific starting or ending letters.\n",
    "* Letters at the beginning or end of the name.\n",
    "* Patterns like doubled letters, etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Putting regexes in their own cell so they only have to be compiled once\n",
    "hyphen_or_space = re.compile(r'[ -]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here's a more complicated version.\n",
    "def gender_features_2(word):\n",
    "    ''' This function should take in a word and return a dictionary\n",
    "        with the name of the feature as the key and the value \n",
    "        as the feature value. '''\n",
    "    ll = word[-1]\n",
    "    penultimate = word[-2]\n",
    "    last_2 = word[-2:]\n",
    "    last_3 = word[-3:]\n",
    "    last_4 = word[-4:]\n",
    "    first_2 = word[:2]\n",
    "        \n",
    "    max_letters = max([v for k,v in Counter(word).items()]) \n",
    "    \n",
    "    if hyphen_or_space.search(word) :\n",
    "        double = True\n",
    "    else :\n",
    "        double = False\n",
    "        \n",
    "    has_bob = \"bob\" in word\n",
    "    \n",
    "    ret_dict = {'last_letter':ll,\n",
    "                'penultimate_y':(penultimate==\"y\"),\n",
    "                'last_3' : last_3,\n",
    "                'last_3_ann_een':(last_3 in {\"ann\",\"een\"}),\n",
    "                'last_4_lynn' : (last_4 == \"lynn\"),\n",
    "                'double_name' : double,\n",
    "                'has_bob' : has_bob,\n",
    "                'first_2':first_2,\n",
    "                'letter_repeats': max_letters >= 2}\n",
    "    \n",
    "    return (ret_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, having defined our new function, we can test it on `devtest`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = [(gender_features_2(n), gender) for (n, gender) in train_names]\n",
    "devtest_set = [(gender_features_2(n), gender) for (n, gender) in devtest_names]\n",
    "test_set = [(gender_features_2(n), gender) for (n, gender) in test_names]\n",
    "\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "print(nltk.classify.accuracy(classifier, devtest_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And you can look at the features and the errors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.show_most_informative_features(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (tag, guess, name) in sorted(errors):\n",
    "    print('correct={:<8} guess={:<8s} name={:<30}'.format(tag, guess, name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Don't run this next cell till you're _completely_ done tweaking your `gender_features_2` code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Once you're done tweaking your code, run this one. \n",
    "print(nltk.classify.accuracy(classifier, test_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So that estimate is your unbiased estimate of your classifier accuracy. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
