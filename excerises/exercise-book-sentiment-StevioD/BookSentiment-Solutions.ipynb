{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis of Plots\n",
    "In this notebook we reprise an [analysis](https://infogram.com/blog/scientists-use-big-data-to-discover-6-basic-emotional-story-arcs/) discussed in _Everybody Lies_. In this chapter Stephens-Davidowitz details an approach to measuring plots, looking at sentiment across a book. He posits six main plots:\n",
    "\n",
    "1. Rags to Riches (rise)\n",
    "1. Riches to Rags (fall)\n",
    "1. Person in a hole (fall-rise)\n",
    "1. Icarus (rise-fall)\n",
    "1. Cinderella (rise-fall-rise)\n",
    "1. Oedipus (fall-rise-fall)\n",
    "\n",
    "In this notebook we'll pick a book from NLTK and do this. (Note that NLTK, by providing digitized copies of many books, has saved us a _lot_ of effort here.) \n",
    "\n",
    "## Getting more books\n",
    "There are about 30 books available from [Project Gutenberg](https://www.gutenberg.org/) via NLTK. There are *many* more available, though. Those books are accessible from a Python package [`gutenberg`](https://pypi.python.org/pypi/Gutenberg). Unfortunately, that package is dependent on an Oracle DB package (BSD-DB), which is a hassle to load. For now, let's roll with NLTK.  \n",
    "\n",
    "## Results from the Paper\n",
    "Here's a graph from the technical results of the analysis:\n",
    "![six story lines](https://about.infogr.am/wp-content/uploads/2016/10/storyarcs-1.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import nltk\n",
    "from nltk.sentiment import SentimentAnalyzer\n",
    "from nltk.sentiment.util import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['austen-emma.txt', 'austen-persuasion.txt', 'austen-sense.txt', 'bible-kjv.txt', 'blake-poems.txt', 'bryant-stories.txt', 'burgess-busterbrown.txt', 'carroll-alice.txt', 'chesterton-ball.txt', 'chesterton-brown.txt', 'chesterton-thursday.txt', 'edgeworth-parents.txt', 'melville-moby_dick.txt', 'milton-paradise.txt', 'shakespeare-caesar.txt', 'shakespeare-hamlet.txt', 'shakespeare-macbeth.txt', 'whitman-leaves.txt']\n",
      "['a_and_c.xml', 'dream.xml', 'hamlet.xml', 'j_caesar.xml', 'macbeth.xml', 'merchant.xml', 'othello.xml', 'r_and_j.xml']\n"
     ]
    }
   ],
   "source": [
    "# Let's start by looking at the books that came along with our NLTK installation\n",
    "print(nltk.corpus.gutenberg.fileids())\n",
    "print(nltk.corpus.shakespeare.fileids())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick one of these and assign it a shorter name.\n",
    "hamlet = nltk.corpus.gutenberg.words(\"shakespeare-hamlet.txt\") # using the \"words\" function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check in at this point in the code--let me know when you're here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(hamlet))\n",
    "print(hamlet[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do sentiment analysis, we're going to need a list of words that indicate positivity or negativity. NLTK has a bunch of this functionality built in. Let's start by playing around with the package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your own sentence here\n",
    "my_sent = \"Going skiing is one of my favorite things about winter--plus all the delicious stews you can make.\"\n",
    "demo_liu_hu_lexicon(my_sent,plot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `demo_liu_hu_lexicon` function does sentiment classification based on the [Liu and Hu opinion lexicon](https://www.cs.uic.edu/~liub/FBS/sentiment-analysis.html). The plotting is optional, but it tells you how the algorithm is working. \n",
    "\n",
    "Another useful set of lexicons comes with `tidytext`, a pretty sweet [book](http://tidytextmining.com/) detailing how to do text mining in R. I've downloaded their sentiment file and put it a text file called `tidytext_sentiments.txt`. This file is tab delimited, with headers and had three columns. The word, the sentiment (\"positive\" or \"negative\"), and the lexicon source of the word. \n",
    "\n",
    "In the cell below, read this file in and create a dictionary. The words will be the key and the value of the dictionary will be `1` if the word is positive and `-1` if the word is negative. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_scores = {}\n",
    "\n",
    "with open(\"tidytext_sentiments.txt\",'r') as infile :\n",
    "    next(infile)\n",
    "    for line in infile.readlines() :\n",
    "        line = line.strip().split(\"\\t\")\n",
    "        if line[1] == \"positive\" :\n",
    "            sentiment_scores[line[0]] = 1\n",
    "        else :\n",
    "            sentiment_scores[line[0]] = -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As always, let's take a look at the data to see if it looks correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, word in enumerate(sentiment_scores) :\n",
    "    print(\"{} has score {}.\".format(word,sentiment_scores[word]))\n",
    "    if idx > 30 :\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks reasonable to me. Now let's do some sentiment scoring. Your book is laid out as a series of words, thanks to the `.words()` function in NLTK. \n",
    "\n",
    "Here's my approach:\n",
    "\n",
    "1. Create an empty list of the same length as your book. \n",
    "1. Set a score variable to 0. \n",
    "1. Iterate over the book, word by word.\n",
    "1. For each word, search for it in our lexicon.\n",
    "1. If the word is in there, change the score variable appropriately \n",
    "1. Store the score in our list. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = [0] * len(hamlet)\n",
    "current_score = 0 \n",
    "\n",
    "for idx, word in enumerate(hamlet) :\n",
    "    if word in sentiment_scores :\n",
    "        current_score += sentiment_scores[word.lower()]\n",
    "    \n",
    "    scores[idx] = current_score\n",
    "    \n",
    "#    if idx > 10000 :\n",
    "#        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll write out our sentiment score to a text file, so that we can plot it in `ggplot` in R. Write this out as two columns, one being the index and the other being the score. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"hamlet_scores.txt\",'w') as ofile :\n",
    "    ofile.write(\"word\\tscore\\n\")\n",
    "    for idx, score in enumerate(scores) :\n",
    "        ofile.write(\"\\t\".join([str(idx+1),str(score)]) + \"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
